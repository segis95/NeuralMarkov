{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy \n",
    "import numpy as np\n",
    "from pyDOE import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import sklearn.metrics\n",
    "import time\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_size = 24\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def Loss(x,y):\n",
    "    return (x - y)**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#sigmoid!!\n",
    "def forecast(w, x, c):\n",
    "\n",
    "    return activation(w[5 * (2 + input_size)] +\\\n",
    "                      (activation( w[5 * input_size: 5 * (input_size + 1)] +\\\n",
    "                                  np.matrix(x) * w[:5 * input_size].reshape(input_size, 5),\\\n",
    "                                  c)).dot(w[5 * (1 + input_size):5 * (2 + input_size)]), c)    \n",
    "    \n",
    "def act1(z):\n",
    "    return (1.0 + np.tanh(z))/2.0\n",
    "    \n",
    "def act2(z):\n",
    "    return ((2.0 * np.arctan(z)/np.pi) + 1.0)/2.0\n",
    "    \n",
    "def activation(z, c):    \n",
    "    if (c == 'arctan'):\n",
    "        return act2(z)\n",
    "    if (c == 'tanh'):\n",
    "        return act1(z)\n",
    "    return sigmoid(z)\n",
    "\n",
    "#Generating the train set and scaling it\n",
    "#For function  (1 + sin(x))/2 \n",
    "\n",
    "#N = number of points\n",
    "#M = number of generated expectations for each x\n",
    "#K = number of instances W for each expectation\n",
    "\n",
    "def learn_network(c, M = 10, K = 20):\n",
    "    \n",
    "    #we generate the training set\n",
    "    #Train_x = np.linspace(-np.pi, np.pi, N)\n",
    "    #Train_x_scaled = np.linspace(-1, 1, N)\n",
    "    #Train_y = ( 1.0 + np.sin(Train_x_scaled)) / 2.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #X\n",
    "    f1 = open('madelon_train.data.txt', 'r')\n",
    "    c = f1.readlines()\n",
    "    #!!!!!!!!!!len(c)\n",
    "    L = [c[i].split(\" \")[:input_size] for i in range(1000)]\n",
    "    L = [[float(L[i][j]) for j in range(len(L[i]))] for i in range(len(L))]\n",
    "    Train_x = np.matrix(L)\n",
    "    f1.close()\n",
    "    \n",
    "    #Y\n",
    "    #function to approximate\n",
    "    f2 = open('madelon_train.labels.txt', 'r')   \n",
    "    \n",
    "    c = f2.readlines()\n",
    "    #len(c)\n",
    "    L = [float(c[i].split(\"\\n\")[0]) for i in range(1000)]\n",
    "    L = [[1.0] if L[i] == 1.0 else [0.0] for i in range(len(L))]\n",
    "    Train_y = np.matrix(L)\n",
    "    f2.close()\n",
    "    \n",
    "    \"\"\"\n",
    "    #*********************************************\n",
    "    #*************OLD DATASET*********************************\n",
    "    \"\"\"\n",
    "    f = open(\"pima-indians-diabetes.data.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(200)]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Train_x = np.matrix([l[i][:8] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][8]] for i in range(len(l))])\n",
    "    #Train_y = ( np.multiply(Train_x,Train_x) * np.matrix([[1],[1]])) / 2.0\n",
    "    #print([y[0,0] == 0.0 for y in Train_y])\n",
    "    \"\"\"\n",
    "    #*****************Second DATASET******************************\n",
    "    #***********************************************\n",
    "    \"\"\"\n",
    "    f = open(\"datatraining.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(1,len(l))]\n",
    "    l = [l[i][0].split(',')[2:8] for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    Train_x = np.matrix([l[i][:5] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][5]] for i in range(len(l))])\n",
    "    \"\"\"\n",
    "    #*****************Third DATASET******************************\n",
    "    #***********************************************\n",
    "    \"\"\"\n",
    "    f = open(\"ionosphere.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(170)]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][34] == 'g'):\n",
    "            l[i][34] = 1.0\n",
    "        else:\n",
    "            l[i][34] = 0.0\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Train_x = np.matrix([l[i][:34] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][34]] for i in range(len(l))])\n",
    "    #Train_y = ( np.multiply(Train_x,Train_x) * np.matrix([[1],[1]])) / 2.0\n",
    "    \n",
    "    if (l[i][24] == '1'):\n",
    "        l[i][24] = 1.0\n",
    "    else:\n",
    "        l[i][24] = 0.0\n",
    "    \"\"\"\n",
    "    #*****************Fourth DATASET******************************\n",
    "    #***********************************************\n",
    "    f = open(\"herman.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(500)]\n",
    "    l = [l[i][0].split(',')[:25] for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][24] == '1'):\n",
    "            l[i][24] = 1.0\n",
    "        else:\n",
    "            l[i][24] = 0.0\n",
    "\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Train_x = np.matrix([l[i][:24] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][24]] for i in range(len(l))])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #we scale dataset or not\n",
    "    scaler = preprocessing.StandardScaler().fit(Train_x);\n",
    "    \n",
    "    #Train_x_scaled = np.array(Train_x.copy())\n",
    "    Train_x_scaled = scaler.transform(Train_x);\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Visualizing dataset\n",
    "    \n",
    "    trues = Train_x_scaled[[y[0,0] == 1.0 for y in Train_y]]\n",
    "    falses = Train_x_scaled[[y[0,0] == 0.0 for y in Train_y]]\n",
    "    \n",
    "    comp_x = 3\n",
    "    comp_y = 4\n",
    "    x_true = [trues[i,comp_x] for i in range(len(trues))]\n",
    "    y_true = [trues[i,comp_y] for i in range(len(trues))]\n",
    "    x_false = [falses[i,comp_x]for i in range(len(falses))]\n",
    "    y_false = [falses[i,comp_y] for i in range(len(falses))]\n",
    "    \n",
    "    \n",
    "    #plt.xlabel('Train LL(random partition) ')\n",
    "    line1 = plt.plot(x_true, y_true, 's',  color = 'r', label = 'True') \n",
    "    line2 = plt.plot(x_false, y_false, '*',  color = 'b', label = 'False')\n",
    "    #plt.xscale('log')\n",
    "    plt.legend(loc='down left')\n",
    "    #plt.savefig('rand_train.png')\n",
    "\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #Dictionary of type {point:best_expectation_of_parameters}\n",
    "    Pairs = {}\n",
    "    \n",
    "    #Disctionary of type {}\n",
    "    Losses = {}\n",
    "    \n",
    "    #Variance parameter\n",
    "    sigma = 1.0\n",
    "    Diag = [[sigma if r==s else 0 for r in range(5 * (2 + input_size) + 1)] for s in range(5 * (2 + input_size) + 1)]\n",
    "    #for each x we find best fitting vector of expectation\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(Train_x_scaled)):\n",
    "        \n",
    "        \n",
    "        if (i % 10 == 0) :\n",
    "            print( \"step \" + str(i) + \" of \" + str(len(Train_x_scaled)) + \"...\")\n",
    "        \n",
    "        #we generate a random set of parameters and scale them to [0,1]^10\n",
    "        \n",
    "        v_set = 2.0 * lhs(5 * (2 + input_size) + 1, M, criterion='maximin') - 1.0\n",
    "        #maps average error to expectation_of_parameters(vector)\n",
    "        error_to_pair = {}\n",
    "         \n",
    "        for j in range(M):#For every vector of parameters(expectation)\n",
    "        \n",
    "            #SECTION 1\n",
    "            \n",
    "            #SetW = [0 for q in range(K)]\n",
    "            \n",
    "            SetW = np.random.multivariate_normal((v_set[j,:]), Diag, K)\n",
    "                       \n",
    "            #losses = [0 for q in range(K)]\n",
    "            \n",
    "            \n",
    "            #for q in range(K):#we calculate loss on each instance of W\n",
    "                #sigmoid!!!   \n",
    "            losses =  map(lambda A: Loss(activation(A[5 * (2 + input_size)] +\\\n",
    "            activation(A[5 * (input_size) : 5 * (1 + input_size)] +\\\n",
    "            np.matrix(Train_x_scaled[i]) * A[ : 5 *  input_size].reshape(input_size,5), c).dot(A[5 * (1\\\n",
    "            + input_size):5*(2 + input_size)]), c), Train_y[i,0]), SetW )\n",
    "                \n",
    "                \n",
    "            #we calculate average error for this instance of expectation vector\n",
    "            Average_error = (np.array(losses)).mean()\n",
    "            \n",
    "            #error -> parameter of exspectation realising it\n",
    "            error_to_pair[Average_error] = v_set[j,:]\n",
    "            \n",
    "            \n",
    "        \n",
    "        #we find the expectation that gives the minimum expected error and attach it the x \n",
    "        min_error = min(error_to_pair.keys())\n",
    "        \n",
    "        Pairs[i] = error_to_pair[min_error]\n",
    "        Losses[i] = min_error   \n",
    "    \n",
    "    s = Pairs[0].copy()\n",
    "        \n",
    "   \n",
    "    #Choice of classifier\n",
    "    #clf = tree.DecisionTreeRegressor(max_depth = 8);\n",
    "    clf = sklearn.linear_model.LinearRegression(fit_intercept=False, normalize = False, copy_X=True)\n",
    "\n",
    "\n",
    "    clf = clf.fit([[Train_x_scaled[i][j] for j in range(input_size)]\\\n",
    "                   for i in range(Train_x_scaled.shape[0])], [Pairs[i] for i in range(Train_x_scaled.shape[0])]);\n",
    "    \n",
    "\n",
    "    \n",
    "    #Printing parameters\n",
    "    numer_of_parameter = 47\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    c1 = 6\n",
    "    c2 = 7\n",
    "    p1 = ax.scatter([x[c1] for x in trues], [x[c2] for x in trues],\\\n",
    "                    [Pairs[i][numer_of_parameter] for i in range(len(Train_x_scaled)) if Train_y[i] == 1.0 ], s = 100, c = \"r\")\n",
    "    \n",
    "    p2 = ax.scatter([x[c1] for x in falses], [x[c2] for x in falses],\\\n",
    "                    [Pairs[i][numer_of_parameter] for i in range(len(Train_x_scaled)) if Train_y[i] == 0.0 ], s = 100, c = \"g\")\n",
    "    \n",
    "    pd = clf.predict([[x[i] for i in range(x.shape[0])] for x in Train_x_scaled])\n",
    "    \n",
    "    p3 = ax.plot_trisurf([Train_x_scaled[i,c1] for i in range(Train_x_scaled.shape[0])],\\\n",
    "                         [Train_x_scaled[i,c2] for i in range(Train_x_scaled.shape[0])],\\\n",
    "                         [pd[i][numer_of_parameter] for i in range(len(Train_x_scaled))])\n",
    "    \n",
    "    ax.set_xlabel(\"x[ \"+ str(c1)+ \" ]\")\n",
    "    ax.set_ylabel(\"x[ \" + str(c2)+ \" ]\")\n",
    "    ax.set_zlabel(\"Parameter\")\n",
    "    ax.set_title(\"Input\" + str(c1) + str(c2)+'Parameter' + str(numer_of_parameter) + '(x)')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.savefig(\"Input\" + str(c1) + str(c2)+'Parameter' + str(numer_of_parameter) + \".jpg\", dpi=1000)\n",
    "    #plt.legend((p1,p2),('Best Parameters', 'Fitted parameters'),numpoints=1, loc='Higer left', ncol=2, fontsize=8,  bbox_to_anchor=(0.8, 0.3))\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test(scaler, Train_x_scaled, Train_y, clf, c, input_size)    \n",
    "    #return (scaler, Train_x_scaled, Train_y, Pairs, clf)\n",
    "        \n",
    "\n",
    "def test(scaler, Train_x_scaled, Train_y, clf, c, input_size):\n",
    "    \n",
    "    #Test_x_1 = np.linspace(-1.0,1.0 ,13)\n",
    "    #Test_x_2 = np.linspace(-1.0, 1.0, 13)\n",
    "    #Test_x = np.matrix([[x,y] for x in Test_x_1 for y in Test_x_2])\n",
    "    #Test_y = ( 1.0 + np.multiply(Test_x,Test_x) * np.matrix([[1],[1]])) / 2.0\n",
    "\n",
    "    #Test_x_scaled = scaler.transform(Test_x);\n",
    "    #Test_x = np.linspace(-np.pi, np.pi, 23)\n",
    "    \n",
    "    #Test_x_scaled = scaler.transform(Test_x)\n",
    "\n",
    "    \n",
    "    #*****************OLD****************************************\n",
    "    #*********************************************************\n",
    "    #********************************************************\n",
    "    \"\"\"\n",
    "    f = open(\"pima-indians-diabetes.data.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(200,len(l))]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]    \n",
    "    \n",
    "    \n",
    "    Test_x = np.matrix([l[i][:8] for i in range(len(l))])\n",
    "    Test_y = np.matrix([[l[i][8]] for i in range(len(l))])\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #*****************NEW DATASET******************************\n",
    "    #***********************************************\n",
    "    \"\"\"\n",
    "    f = open(\"datatest.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(1,len(l))]\n",
    "    l = [l[i][0].split(',')[2:8] for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    Test_x = np.matrix([l[i][:5] for i in range(len(l))])\n",
    "    Test_y = np.matrix([[l[i][5]] for i in range(len(l))])\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #***************Third dataset******************\n",
    "    #*********************************************\n",
    "    \"\"\"\n",
    "    f = open(\"ionosphere.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(170,len(l))]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][34] == 'g'):\n",
    "            l[i][34] = 1.0\n",
    "        else:\n",
    "            l[i][34] = 0.0\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \"\"\"\n",
    "       #*****************Fourth DATASET******************************\n",
    "    #***********************************************\n",
    "    f = open(\"herman.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(500,1000)]\n",
    "    l = [l[i][0].split(',')[:25] for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][24] == '1'):\n",
    "            l[i][24] = 1.0\n",
    "        else:\n",
    "            l[i][24] = 0.0\n",
    "\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Test_x = np.matrix([l[i][:24] for i in range(len(l))])\n",
    "    Test_y = np.matrix([[l[i][24]] for i in range(len(l))])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Test_x_scaled = np.array(Test_x.copy())\n",
    "    Test_x_scaled = scaler.transform(Test_x);\n",
    "    \n",
    "    \n",
    "    pd = clf.predict([[x[i] for i in range(x.shape[0])] for x in Test_x_scaled])\n",
    "    \n",
    "    \n",
    "    predictions = np.zeros(Test_x.shape[0])\n",
    "    \n",
    "    for i in range(Test_x.shape[0]):\n",
    "        predictions[i] = forecast(pd[i], Test_x_scaled[i], c)\n",
    "    \"\"\"\n",
    "    p1 = [Train_x_scaled[i,1] for i in range(len(Train_x_scaled))]\n",
    "    p2 = [Train_x_scaled[i,3] for i in range(len(Train_x_scaled))]\n",
    "    yp = [Train_y[i,0] for i in range(len(Train_y))]\n",
    "    print(yp[1])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    #q1 = ax.plot_trisurf([Train_x_scaled[i,0] for i in range(Train_x_scaled.shape[0])],\\\n",
    "    [Train_x_scaled[i,1] for i in range(Train_x.shape[0])], [Pairs[i][numer_of_parameter] for i in range(len(Pairs.keys()))], s = 3, c = \"g\")\n",
    "    q2 = ax.scatter(p1,p2,yp,cmap=cm.bone, alpha=0.05)\n",
    "    plt.plot()    \n",
    "    \"\"\"\n",
    "    print(\"LogLoss per unit is \" + str(sklearn.metrics.log_loss(Test_y, predictions, normalize = True)));\n",
    "    \n",
    "    predictions = [1  if predictions[i] > 0.5 else 0 for i in range(len(predictions))]\n",
    "    \n",
    "    print(\"Accuracy is \" + str(sklearn.metrics.accuracy_score(Test_y, predictions, normalize = True)));\n",
    "    \n",
    "\n",
    "    #print(\"Predictions \" + c + \"!********************\")\n",
    "    #for i in range(Test_x_scaled.shape[0]):\n",
    "    #    print(predictions[i])\n",
    "    \n",
    "    \n",
    "\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    #ax.plot_trisurf([Train_x_scaled[i][0] for i in range(Train_x_scaled.shape[0])],\\\n",
    "    #[Train_x_scaled[i][1] for i in range(Train_x_scaled.shape[0])],[x[0,0] for x in Train_y],cmap=cm.bone, alpha=0.4 )\n",
    "    #ax.scatter([Test_x_scaled[i][0] for i in range(Test_x_scaled.shape[0])],[Test_x_scaled[i][1] for i in range(Test_x_scaled.shape[0])],predictions, s = 3, c = \"g\")\n",
    "    #fig.savefig(c + \"3d.png\")\n",
    "    #plt.plot(Test_x_scaled, predictions,'o', Train_x_scaled, Train_y)\n",
    "    #plt.title(\"Activation function: \" + c)\n",
    "    #plt.savefig(c + \"3d.png\")\n",
    "\n",
    "def read_data():\n",
    "    \n",
    "    f = open('madelon_train.data.txt', 'r')\n",
    "    \n",
    "    c = f.readlines()\n",
    "    L = [c[i].split(\" \")[:500] for i in range(len(c))]\n",
    "    L = [[float(L[i][j]) for j in range(len(L[i]))] for i in range(len(L))]\n",
    "    \n",
    "    \n",
    "    #for y\n",
    "    L = [float(c[i].split(\"\\n\")[0]) for i in range(len(c))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def network(c = \"cls\"):\n",
    "    \n",
    "\n",
    "    \n",
    "    #Train_y = ( np.multiply(Train_x,Train_x) * np.matrix([[1],[1]])) / 2.0\n",
    "    #Train_x_scaled = np.array(Train_x.copy())\n",
    "    \n",
    "    #First\n",
    "    \"\"\"\n",
    "    f = open(\"pima-indians-diabetes.data.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(100)]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    Train_x = np.matrix([l[i][:8] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][8]] for i in range(len(l))])\n",
    "    f.close()\n",
    "    \"\"\"\n",
    "    #Second\n",
    "    \"\"\"\n",
    "    f = open(\"datatest.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(1,len(l))]\n",
    "    l = [l[i][0].split(',')[2:8] for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    Train_x = np.matrix([l[i][:5] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][5]] for i in range(len(l))])\n",
    "    f.close()\n",
    "    \"\"\"\n",
    "    #Third\n",
    "    \"\"\"\n",
    "    f = open(\"ionosphere.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(170)]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][34] == 'g'):\n",
    "            l[i][34] = 1.0\n",
    "        else:\n",
    "            l[i][34] = 0.0\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    f.close()\n",
    "    \"\"\"\n",
    "    #*****************Fourth DATASET******************************\n",
    "    #***********************************************\n",
    "    f = open(\"herman.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(0,500)]\n",
    "    l = [l[i][0].split(',')[:25] for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][24] == '1'):\n",
    "            l[i][24] = 1.0\n",
    "        else:\n",
    "            l[i][24] = 0.0\n",
    "\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Train_x = np.matrix([l[i][:24] for i in range(len(l))])\n",
    "    Train_y = np.matrix([[l[i][24]] for i in range(len(l))])\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(Train_x)\n",
    "    \n",
    "    \n",
    "    #we scale dataset or not\n",
    "    #Test_x_scaled = np.array(Test_x.copy())\n",
    "    Train_x_scaled = scaler.transform(Train_x)\n",
    "    \n",
    "    if (c == \"reg\"):\n",
    "        clf = MLPRegressor(activation='tanh', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5))\n",
    "    else:\n",
    "        clf = MLPClassifier(activation='tanh', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5))\n",
    "    \n",
    "    clf.fit(Train_x_scaled, Train_y)\n",
    "    \n",
    "    #OLD\n",
    "    \"\"\"\n",
    "    f = open(\"pima-indians-diabetes.data.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(100, len(l))]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Test_x = np.matrix([l[i][:8] for i in range(len(l))])\n",
    "    Test_y = np.matrix([[l[i][8]] for i in range(len(l))])\n",
    "    f.close() \n",
    "    \"\"\"\n",
    "    #NEW\n",
    "    \"\"\"\n",
    "    f = open(\"datatest.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(1,len(l))]\n",
    "    l = [l[i][0].split(',')[2:8] for i in range(len(l))]\n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    Test_x = np.matrix([l[i][:5] for i in range(len(l))])\n",
    "    Test_y = np.matrix([[l[i][5]] for i in range(len(l))])\n",
    "    f.close() \n",
    "    \"\"\"\n",
    "    #Third\n",
    "    \"\"\"\n",
    "    f = open(\"ionosphere.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(170,len(l))]\n",
    "    l = [l[i][0].split(',') for i in range(len(l))]\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        if (l[i][34] == 'g'):\n",
    "            l[i][34] = 1.0\n",
    "        else:\n",
    "            l[i][34] = 0.0\n",
    "\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    f.close()\n",
    "    \"\"\"\n",
    "    f = open(\"herman.txt\",'r')\n",
    "    l = f.readlines()\n",
    "    l = [l[i].split(\"\\n\") for i in range(500,1000)]\n",
    "    l = [l[i][0].split(',')[:25] for i in range(len(l))]\n",
    "    for i in range(len(l)):\n",
    "        if (l[i][24] == '1'):\n",
    "            l[i][24] = 1.0\n",
    "        else:\n",
    "            l[i][24] = 0.0\n",
    "\n",
    "        \n",
    "    l = [[float(l[i][j]) for j in range(len(l[i]))] for i in range(len(l))]\n",
    "    \n",
    "    Test_x = np.matrix([l[i][:24] for i in range(len(l))])\n",
    "    Test_y = np.matrix([[l[i][24]] for i in range(len(l))])\n",
    "    f.close() \n",
    "    \n",
    "    \n",
    "    \n",
    "    #We scale dataset or not\n",
    "    #Test_x_scaled = np.array(Test_x.copy())\n",
    "    Test_x_scaled = scaler.transform(Test_x)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction = clf.predict(Test_x_scaled)\n",
    "    \n",
    "    #print(\"Precision is \" + str(sklearn.metrics.accuracy_score(Test_y, prediction, normalize = True)))\n",
    "    if (c == \"reg\"):\n",
    "        print(\"Logloss is \" + str(sklearn.metrics.log_loss(Test_y, prediction, normalize = True)))\n",
    "    else:\n",
    "        print(\"Accuracy is  \" + str(sklearn.metrics.accuracy_score(Test_y, prediction, normalize = True)))\n",
    "        \n",
    "    \n",
    "    #print(clf.coefs_[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 of 500...\n",
      "step 10 of 500...\n",
      "step 20 of 500...\n",
      "step 30 of 500...\n",
      "step 40 of 500...\n",
      "step 50 of 500...\n",
      "step 60 of 500...\n",
      "step 70 of 500...\n",
      "step 80 of 500...\n",
      "step 90 of 500...\n",
      "step 100 of 500...\n",
      "step 110 of 500...\n",
      "step 120 of 500...\n",
      "step 130 of 500...\n",
      "step 140 of 500...\n",
      "step 150 of 500...\n",
      "step 160 of 500...\n",
      "step 170 of 500...\n",
      "step 180 of 500...\n",
      "step 190 of 500...\n",
      "step 200 of 500...\n",
      "step 210 of 500...\n",
      "step 220 of 500...\n",
      "step 230 of 500...\n",
      "step 240 of 500...\n",
      "step 250 of 500...\n",
      "step 260 of 500...\n",
      "step 270 of 500...\n",
      "step 280 of 500...\n",
      "step 290 of 500...\n",
      "step 300 of 500...\n",
      "step 310 of 500...\n",
      "step 320 of 500...\n",
      "step 330 of 500...\n",
      "step 340 of 500...\n",
      "step 350 of 500...\n",
      "step 360 of 500...\n",
      "step 370 of 500...\n",
      "step 380 of 500...\n",
      "step 390 of 500...\n",
      "step 400 of 500...\n",
      "step 410 of 500...\n",
      "step 420 of 500...\n",
      "step 430 of 500...\n",
      "step 440 of 500...\n",
      "step 450 of 500...\n",
      "step 460 of 500...\n",
      "step 470 of 500...\n",
      "step 480 of 500...\n",
      "step 490 of 500...\n",
      "LogLoss per unit is 0.605450101576\n",
      "Accuracy is 0.7\n"
     ]
    }
   ],
   "source": [
    "learn_network(\"arctan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([True, False])\n",
    "s/True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.7\n"
     ]
    }
   ],
   "source": [
    "network(c = \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
